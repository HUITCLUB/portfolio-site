---
layout: post
title:  "Sequence to Sequence Learning with Neural Networks"
date: 2018-02-27
category: [papers]
tags: [RNN, LSTM, Seq2Seq]
---

https://arxiv.org/abs/1409.3215

## 1. Introduction
Seq2Seq は入力ベクトルを受け取って異なる次元のベクトルを出力する LSTM の構造であり、
DNNs にとって困難である、可変的な出力を得ることを可能とした。
翻訳の場合、まず 1 つ目の LSTM で入力ベクトル（各単語）を逆順に受け取り、そこから得られ
る文章の意味を表現する固定次元のベクトル(the fixed- dimensional representation $$\boldsymbol{v}$$ )を用いて、2 つ目
の LSTM で出力する可変長のベクトル（翻訳語の単語の列）を生成する。

## 2. The Model
![Figure1](https://huitclub.github.io/images/seq2seq.jpg "Seq2Seq")

基本的な構造は Figure1 の通りだが、以下の 3 点が違う。

1. 前半と後半で 2 つの LSTM を使用する。これにより、計算コストを無視できるほど多くのパラ
メータも持たせられる。
2. Deep LSTM の方が Shallow LSTM よりも性能がいいので、4 層の LSTM を使用する。
3. 入力ベクトルを逆順にする。

## 3. Experiments

### 3.1 使用するデータ
WMT’14 English to French dataset を使用し、16 万単語をソース文章に、8 万単語を翻訳文章に適用
した。除外された単語は全て”<UNK>”というトークンに置き換えた。

### 3.2 デコード方法
デコードには beam search decoder を用いて、単語の部分仮説の内、確率の高い数個を残して、他
の可能性を排除するという操作を行った。

### 3.3 入力ベクトルを逆順に
こうすると精度が向上する厳密な理由はわからないが、minimal time lag 問題を解決し、誤差逆伝
播にてインプットとアウトプット間の communication の確立を容易にした。

### 3.4 詳細設定
- 重みの初期値$$\sim \mathcal{U} (−0.08, 0.08)$$
- SGD を使用し、最初の 5epoch は $$\mu=0.7$$でそこから 0.5epoch ごとに半減させ、7.5epochs まで学
習させた。
- 128個のバッチを使用し、勾配を128で割った。
- 勾配爆発問題を防ぐために、は勾配を128で割ったものを$$g$$とし、$$ s = \left\| g\right\| ^2 $$を計算し、$$s > 5$$の時、 $$ g = \dfrac{s}{5g} $$とする。

$$
\begin{align*}
  & \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
  = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
  & (x_1, \ldots, x_n) \left( \begin{array}{ccc}
      \phi(e_1, e_1) & \cdots & \phi(e_1, e_n) \\
      \vdots & \ddots & \vdots \\
      \phi(e_n, e_1) & \cdots & \phi(e_n, e_n)
    \end{array} \right)
  \left( \begin{array}{c}
      y_1 \\
      \vdots \\
      y_n
    \end{array} \right)
\end{align*}
$$


### 3.5 実験結果
BLEU スコアにおいて、長文でも高い精度が得られた。

## 4. Related Work

## 5. Conclusion
この実験で、制限された語彙と問題仮定のない Deep LSTM が、語彙制限のない大規模の標準的な
統計的機械学習モデルを上回ったことを示した。
実験者が驚いた点は 2 つ。

- 入力ベクトルを逆順に入れることで、精度が飛躍的に向上した点。
- 長い文章の翻訳も先行研究と違い、逆順にしたことで高い精度が得られた点。

### References
LSTM formulation
[10] A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint arXiv:1308.0850,
2013.

Minimal time lag 問題
[17] S. Hochreiter and J. Schmidhuber. LSTM can solve hard long time lag problems. 1997
